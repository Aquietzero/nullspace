---
layout: post
title: AI学习时间 08 - Transformer 与自注意力机制 3
date: 2025-05-13
categories:
    - AI
tags:
    - AI
    - 学习
    - Transformer
    - 自注意力机制
    - 大语言模型架构
---

<div class="theme-color-blue" markdown=1>
`#大语言模型` `#Transformer` `#自注意力机制`
</div>

# 复习

- **Transformer 架构**：对输入进行了理解（编码），再把理解转化为输出（解码）。
- **理解**：所谓的“理解”，实际上是一个高维稠密的向量空间。

# 稠密向量空间

把字符转化为向量可以说是一个很天才的设计，通过这样的方式，离散的字符转化为连续的向量。这种设计有几个重要考虑，回忆之前提到过的概念：

- 大模型的输入输出是向量（张量）
- Transformer 架构本质是一个函数，函数内部是向量与矩阵的运算
- 连续变量才能求导，进行反向传播，进行学习训练
- 连续变量可以构成一个度量空间，可以进行距离计算

大语言模型通过 